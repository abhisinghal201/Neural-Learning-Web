"""
Neural Odyssey - Week 38: Paper Writing & Publication Excellence
Phase 4: Mastery and Innovation (Publication Mastery Track)

Academic Writing and Research Communication Mastery

This week transforms your research discoveries into influential publications that shape the AI field.
You'll master the art of scientific storytelling, navigate the peer review process with expertise,
and establish yourself as a thought leader through compelling research communication.

Learning Objectives:
- Master academic writing for high-impact AI research papers
- Develop compelling research narratives that engage and influence readers
- Navigate the peer review process with professionalism and strategic thinking
- Build systematic publication strategies for maximum career impact
- Create reproducible research packages that maximize scholarly influence
- Establish thought leadership through excellent research communication
- Transform technical discoveries into accessible scientific stories

Author: Neural Explorer & Academic Writing Excellence Community
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Academic Writing and Publication Tools
import re
import string
import nltk
import spacy
from textstat import flesch_reading_ease, flesch_kincaid_grade
from wordcloud import WordCloud
import bibtexparser
from bibtexparser.bib import BibDataBase
from bibtexparser.writer import BibTexWriter

# Research Analysis and Visualization
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.manifold import TSNE
import networkx as nx
from community import community_louvain

# LaTeX and Document Generation
from pylatex import Document, Section, Subsection, Command, Figure, Table
from pylatex.base_classes import Environment
from pylatex.utils import NoEscape, italic, bold
from pylatex.package import Package
import jinja2

# Publication Metrics and Analysis
import requests
import json
from scholarly import scholarly
import arxiv
import feedparser

# Collaboration and Review Management
import git
from github import Github
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

from typing import Dict, List, Optional, Union, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')


# ==========================================
# ACADEMIC WRITING MASTERY FRAMEWORK
# ==========================================

class AcademicWritingFramework:
    """
    Comprehensive framework for mastering academic writing and publication excellence
    Features: Paper structure optimization, narrative development, peer review navigation
    """
    
    def __init__(self):
        self.paper_projects = {}
        self.writing_analytics = {}
        self.review_management = {}
        self.publication_strategy = {}
        
        # Initialize writing tools
        self.writing_assistant = WritingAssistant()
        self.narrative_developer = NarrativeDeveloper()
        self.review_navigator = PeerReviewNavigator()
        
    def develop_compelling_research_narrative(self):
        """
        Develop compelling research narratives that engage readers and maximize impact
        Focus: Storytelling techniques that transform technical work into influential papers
        """
        print("ðŸ“– Developing Compelling Research Narratives: The Art of Scientific Storytelling")
        print("=" * 80)
        
        class ResearchNarrativeDeveloper:
            def __init__(self):
                self.narrative_frameworks = {}
                self.story_structures = {}
                self.writing_techniques = {}
                self.impact_optimization = {}
                
            def analyze_breakthrough_paper_structures(self):
                """Analyze structure of breakthrough AI papers for narrative patterns"""
                print("ðŸ” Analyzing Breakthrough Paper Structures...")
                
                # Analyze famous AI papers for narrative patterns
                breakthrough_papers = {
                    'attention_is_all_you_need': {
                        'title': 'Attention Is All You Need',
                        'authors': 'Vaswani et al.',
                        'venue': 'NeurIPS 2017',
                        'narrative_structure': {
                            'hook': 'Bold claim challenging existing paradigm',
                            'problem': 'Computational limitations of RNNs and CNNs',
                            'solution': 'Pure attention mechanism without recurrence',
                            'validation': 'Superior performance with faster training',
                            'impact': 'Foundation for modern NLP revolution'
                        },
                        'writing_techniques': [
                            'Provocative title with strong claim',
                            'Clear problem-solution framing',
                            'Systematic ablation studies',
                            'Comprehensive comparative evaluation',
                            'Strong conclusion with broad implications'
                        ],
                        'success_factors': [
                            'Timing aligned with computational needs',
                            'Clear presentation of complex ideas',
                            'Thorough experimental validation',
                            'Code and model release',
                            'Strong academic-industry collaboration'
                        ]
                    },
                    'resnet_paper': {
                        'title': 'Deep Residual Learning for Image Recognition',
                        'authors': 'He et al.',
                        'venue': 'CVPR 2016',
                        'narrative_structure': {
                            'hook': 'Deeper networks perform worse - counterintuitive finding',
                            'problem': 'Degradation problem in very deep networks',
                            'solution': 'Residual connections and identity mappings',
                            'validation': 'Record-breaking ImageNet performance',
                            'impact': 'Enabled training of very deep networks'
                        },
                        'writing_techniques': [
                            'Counterintuitive observation as hook',
                            'Clear problem identification with evidence',
                            'Simple yet powerful solution',
                            'Extensive experimental validation',
                            'Theoretical analysis of solution'
                        ],
                        'success_factors': [
                            'Addressed fundamental architectural limitation',
                            'Simple idea with broad applicability',
                            'Comprehensive experimental evaluation',
                            'Clear improvement over baselines',
                            'Theoretical understanding provided'
                        ]
                    },
                    'gan_paper': {
                        'title': 'Generative Adversarial Networks',
                        'authors': 'Goodfellow et al.',
                        'venue': 'NeurIPS 2014',
                        'narrative_structure': {
                            'hook': 'Game-theoretic approach to generative modeling',
                            'problem': 'Difficulty in training generative models',
                            'solution': 'Adversarial training framework',
                            'validation': 'Qualitative and quantitative evaluation',
                            'impact': 'New paradigm for generative modeling'
                        },
                        'writing_techniques': [
                            'Novel conceptual framework introduction',
                            'Mathematical formulation with intuition',
                            'Connection to game theory concepts',
                            'Multiple evaluation approaches',
                            'Discussion of limitations and future work'
                        ],
                        'success_factors': [
                            'Novel conceptual contribution',
                            'Strong theoretical foundation',
                            'Practical algorithmic framework',
                            'Extensible research direction',
                            'Clear presentation of complex ideas'
                        ]
                    }
                }
                
                # Extract common narrative patterns
                narrative_patterns = self.extract_narrative_patterns(breakthrough_papers)
                
                # Visualize narrative structure analysis
                self.visualize_narrative_analysis(breakthrough_papers, narrative_patterns)
                
                self.story_structures['breakthrough_analysis'] = {
                    'papers': breakthrough_papers,
                    'patterns': narrative_patterns
                }
                
                print("âœ… Breakthrough paper narrative structures analyzed")
                return narrative_patterns
                
            def extract_narrative_patterns(self, papers):
                """Extract common patterns from breakthrough papers"""
                
                patterns = {
                    'opening_strategies': {
                        'bold_claim': 'Make a provocative statement that challenges assumptions',
                        'counterintuitive_finding': 'Present surprising observation that demands explanation',
                        'gap_identification': 'Identify clear limitation in current approaches',
                        'paradigm_shift': 'Introduce fundamentally new way of thinking'
                    },
                    'problem_framing': {
                        'quantified_limitation': 'Provide specific metrics showing current limitations',
                        'theoretical_gap': 'Identify gap between theory and practice',
                        'scalability_challenge': 'Show how current methods fail at scale',
                        'performance_bottleneck': 'Identify specific performance constraints'
                    },
                    'solution_presentation': {
                        'elegant_simplicity': 'Present simple solution to complex problem',
                        'unified_framework': 'Provide single framework addressing multiple issues',
                        'theoretical_grounding': 'Connect solution to solid theoretical foundation',
                        'practical_algorithm': 'Provide concrete algorithmic implementation'
                    },
                    'validation_strategies': {
                        'comprehensive_evaluation': 'Test across multiple datasets and metrics',
                        'ablation_analysis': 'Systematically analyze component contributions',
                        'comparative_study': 'Compare against multiple strong baselines',
                        'theoretical_analysis': 'Provide mathematical analysis of properties'
                    },
                    'impact_communication': {
                        'broad_applicability': 'Show solution applies beyond immediate problem',
                        'future_research': 'Identify promising research directions enabled',
                        'practical_implications': 'Discuss real-world applications and benefits',
                        'theoretical_contributions': 'Highlight advances in understanding'
                    }
                }
                
                return patterns
                
            def visualize_narrative_analysis(self, papers, patterns):
                """Create visualizations of narrative structure analysis"""
                
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                
                # Narrative flow comparison
                paper_names = list(papers.keys())
                flow_elements = ['hook', 'problem', 'solution', 'validation', 'impact']
                
                for i, paper_name in enumerate(paper_names):
                    paper = papers[paper_name]
                    narrative = paper['narrative_structure']
                    
                    # Create narrative flow diagram
                    x_pos = range(len(flow_elements))
                    y_pos = [1] * len(flow_elements)  # Same height for flow
                    
                    axes[0, i].plot(x_pos, y_pos, 'o-', linewidth=3, markersize=10)
                    
                    for j, element in enumerate(flow_elements):
                        axes[0, i].annotate(element.title(), (j, 1), 
                                           xytext=(0, 20), textcoords='offset points',
                                           ha='center', fontsize=8, rotation=45)
                        
                        # Add brief description
                        description = narrative[element][:30] + '...' if len(narrative[element]) > 30 else narrative[element]
                        axes[0, i].annotate(description, (j, 1), 
                                           xytext=(0, -30), textcoords='offset points',
                                           ha='center', fontsize=6, rotation=0,
                                           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))
                    
                    axes[0, i].set_title(f"{paper['title'][:20]}...", fontsize=10)
                    axes[0, i].set_xlim(-0.5, len(flow_elements) - 0.5)
                    axes[0, i].set_ylim(0.5, 1.8)
                    axes[0, i].set_xticks([])
                    axes[0, i].set_yticks([])
                
                # Success factors analysis
                all_factors = []
                for paper in papers.values():
                    all_factors.extend(paper['success_factors'])
                
                # Count factor frequency
                factor_counts = {}
                for factor in all_factors:
                    factor_counts[factor] = factor_counts.get(factor, 0) + 1
                
                factors = list(factor_counts.keys())[:10]  # Top 10
                counts = [factor_counts[f] for f in factors]
                
                axes[1, 0].barh(factors, counts, color='lightgreen', alpha=0.7)
                axes[1, 0].set_title('Common Success Factors')
                axes[1, 0].set_xlabel('Frequency')
                
                # Writing techniques analysis
                all_techniques = []
                for paper in papers.values():
                    all_techniques.extend(paper['writing_techniques'])
                
                technique_counts = {}
                for technique in all_techniques:
                    technique_counts[technique] = technique_counts.get(technique, 0) + 1
                
                techniques = list(technique_counts.keys())[:8]  # Top 8
                tech_counts = [technique_counts[t] for t in techniques]
                
                axes[1, 1].bar(range(len(techniques)), tech_counts, color='lightcoral', alpha=0.7)
                axes[1, 1].set_title('Writing Techniques Used')
                axes[1, 1].set_xticks(range(len(techniques)))
                axes[1, 1].set_xticklabels([t[:15] + '...' for t in techniques], rotation=45, ha='right')
                axes[1, 1].set_ylabel('Frequency')
                
                # Impact timeline visualization
                venues = [papers[p]['venue'] for p in paper_names]
                years = [int(venue.split()[-1]) for venue in venues]
                citations = [15000, 12000, 8000]  # Simulated citation counts
                
                axes[1, 2].scatter(years, citations, s=200, alpha=0.7, c=['red', 'blue', 'green'])
                for i, paper_name in enumerate(paper_names):
                    axes[1, 2].annotate(papers[paper_name]['title'][:15] + '...', 
                                       (years[i], citations[i]),
                                       xytext=(5, 5), textcoords='offset points', fontsize=8)
                
                axes[1, 2].set_xlabel('Publication Year')
                axes[1, 2].set_ylabel('Citations (Simulated)')
                axes[1, 2].set_title('Impact Over Time')
                axes[1, 2].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
            def create_narrative_frameworks(self):
                """Create frameworks for different types of research narratives"""
                print("ðŸ“š Creating Research Narrative Frameworks...")
                
                narrative_frameworks = {
                    'problem_solution_framework': {
                        'description': 'Classic problem-solution narrative structure',
                        'structure': {
                            'title': 'Clear indication of problem and solution approach',
                            'abstract': {
                                'motivation': 'Why this problem matters (1-2 sentences)',
                                'problem': 'Specific challenge addressed (1 sentence)',
                                'approach': 'Key methodological innovation (2-3 sentences)',
                                'results': 'Main quantitative findings (1-2 sentences)',
                                'impact': 'Broader significance (1 sentence)'
                            },
                            'introduction': {
                                'hook': 'Compelling opening that establishes importance',
                                'context': 'Background and motivation for the problem',
                                'gap': 'Limitations of current approaches',
                                'contribution': 'Clear statement of novel contributions',
                                'roadmap': 'Organization of the paper'
                            },
                            'related_work': {
                                'taxonomy': 'Organize existing work into clear categories',
                                'analysis': 'Critical analysis of strengths and limitations',
                                'positioning': 'Clear positioning of current work'
                            },
                            'method': {
                                'overview': 'High-level approach description',
                                'details': 'Technical details with sufficient clarity',
                                'innovation': 'Highlight novel aspects clearly',
                                'implementation': 'Key implementation considerations'
                            },
                            'experiments': {
                                'setup': 'Comprehensive experimental design',
                                'baselines': 'Strong and fair baseline comparisons',
                                'metrics': 'Appropriate evaluation metrics',
                                'analysis': 'Thorough analysis of results'
                            },
                            'conclusion': {
                                'summary': 'Concise summary of contributions',
                                'impact': 'Broader implications and significance',
                                'future': 'Promising future research directions'
                            }
                        },
                        'best_for': ['Algorithmic contributions', 'Method development', 'System improvements']
                    },
                    'discovery_framework': {
                        'description': 'Narrative for presenting surprising discoveries or insights',
                        'structure': {
                            'title': 'Intriguing statement that raises questions',
                            'opening': 'Counterintuitive finding or surprising observation',
                            'investigation': 'Systematic investigation of the phenomenon',
                            'explanation': 'Theoretical or empirical explanation',
                            'validation': 'Independent validation of findings',
                            'implications': 'Broader implications for the field'
                        },
                        'techniques': [
                            'Start with surprising or counterintuitive finding',
                            'Use data and visualizations to support claims',
                            'Provide clear explanations for unexpected results',
                            'Connect findings to broader theoretical framework',
                            'Discuss implications for future research'
                        ],
                        'best_for': ['Empirical studies', 'Analysis papers', 'Unexpected findings']
                    },
                    'unification_framework': {
                        'description': 'Narrative for papers that unify disparate approaches',
                        'structure': {
                            'motivation': 'Show fragmentation in current approaches',
                            'analysis': 'Analyze commonalities and differences',
                            'framework': 'Present unified theoretical framework',
                            'instantiation': 'Show how existing methods fit framework',
                            'novel_insights': 'New insights enabled by unification',
                            'validation': 'Empirical validation of unified approach'
                        },
                        'techniques': [
                            'Identify common underlying principles',
                            'Provide clear theoretical foundation',
                            'Show how framework explains existing results',
                            'Demonstrate practical benefits of unification',
                            'Enable new research directions'
                        ],
                        'best_for': ['Theoretical contributions', 'Survey papers', 'Framework papers']
                    }
                }
                
                self.narrative_frameworks = narrative_frameworks
                
                print("âœ… Research narrative frameworks created")
                return narrative_frameworks
                
            def develop_writing_optimization_tools(self):
                """Develop tools for optimizing research writing quality"""
                print("ðŸ› ï¸ Developing Writing Optimization Tools...")
                
                class WritingOptimizer:
                    def __init__(self):
                        self.style_checkers = {}
                        self.readability_analyzers = {}
                        self.clarity_enhancers = {}
                        
                    def analyze_writing_quality(self, text):
                        """Comprehensive analysis of writing quality"""
                        
                        analysis = {
                            'readability_metrics': self.calculate_readability_metrics(text),
                            'style_analysis': self.analyze_writing_style(text),
                            'clarity_assessment': self.assess_clarity(text),
                            'engagement_factors': self.analyze_engagement(text),
                            'technical_precision': self.assess_technical_precision(text)
                        }
                        
                        return analysis
                        
                    def calculate_readability_metrics(self, text):
                        """Calculate various readability metrics"""
                        
                        metrics = {
                            'flesch_reading_ease': flesch_reading_ease(text),
                            'flesch_kincaid_grade': flesch_kincaid_grade(text),
                            'average_sentence_length': self.average_sentence_length(text),
                            'average_word_length': self.average_word_length(text),
                            'technical_term_density': self.technical_term_density(text)
                        }
                        
                        # Interpret scores for academic writing
                        metrics['readability_assessment'] = self.interpret_readability(metrics)
                        
                        return metrics
                        
                    def average_sentence_length(self, text):
                        """Calculate average sentence length"""
                        sentences = nltk.sent_tokenize(text)
                        if not sentences:
                            return 0
                        
                        total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)
                        return total_words / len(sentences)
                        
                    def average_word_length(self, text):
                        """Calculate average word length"""
                        words = nltk.word_tokenize(text)
                        if not words:
                            return 0
                        
                        total_chars = sum(len(word) for word in words if word.isalpha())
                        alpha_words = [word for word in words if word.isalpha()]
                        
                        return total_chars / len(alpha_words) if alpha_words else 0
                        
                    def technical_term_density(self, text):
                        """Calculate density of technical terms"""
                        words = nltk.word_tokenize(text.lower())
                        
                        # Common ML/AI technical terms
                        technical_terms = {
                            'algorithm', 'model', 'neural', 'network', 'learning', 'training',
                            'optimization', 'gradient', 'loss', 'function', 'parameter',
                            'hyperparameter', 'accuracy', 'precision', 'recall', 'f1',
                            'dataset', 'batch', 'epoch', 'validation', 'test', 'baseline',
                            'transformer', 'attention', 'embedding', 'encoder', 'decoder'
                        }
                        
                        technical_count = sum(1 for word in words if word in technical_terms)
                        return technical_count / len(words) if words else 0
                        
                    def interpret_readability(self, metrics):
                        """Interpret readability metrics for academic writing"""
                        
                        flesch_score = metrics['flesch_reading_ease']
                        
                        if flesch_score >= 60:
                            readability_level = 'Very Accessible'
                            recommendation = 'Good for broad audience, consider if too simple for experts'
                        elif flesch_score >= 50:
                            readability_level = 'Accessible'
                            recommendation = 'Appropriate for academic audience'
                        elif flesch_score >= 30:
                            readability_level = 'Moderate'
                            recommendation = 'Standard for technical papers'
                        else:
                            readability_level = 'Difficult'
                            recommendation = 'May be too complex, consider simplifying'
                        
                        return {
                            'level': readability_level,
                            'recommendation': recommendation,
                            'flesch_score': flesch_score
                        }
                        
                    def analyze_writing_style(self, text):
                        """Analyze writing style characteristics"""
                        
                        sentences = nltk.sent_tokenize(text)
                        words = nltk.word_tokenize(text)
                        
                        style_metrics = {
                            'sentence_variety': self.calculate_sentence_variety(sentences),
                            'passive_voice_ratio': self.passive_voice_ratio(text),
                            'transition_usage': self.analyze_transitions(text),
                            'paragraph_structure': self.analyze_paragraph_structure(text),
                            'voice_consistency': self.analyze_voice_consistency(text)
                        }
                        
                        return style_metrics
                        
                    def calculate_sentence_variety(self, sentences):
                        """Calculate variety in sentence structure"""
                        if not sentences:
                            return 0
                            
                        sentence_lengths = [len(nltk.word_tokenize(s)) for s in sentences]
                        
                        # Calculate coefficient of variation
                        mean_length = np.mean(sentence_lengths)
                        std_length = np.std(sentence_lengths)
                        
                        variety_score = std_length / mean_length if mean_length > 0 else 0
                        
                        return {
                            'variety_score': variety_score,
                            'mean_length': mean_length,
                            'length_range': (min(sentence_lengths), max(sentence_lengths)),
                            'assessment': 'Good variety' if variety_score > 0.3 else 'Consider varying sentence length'
                        }
                        
                    def passive_voice_ratio(self, text):
                        """Estimate passive voice usage ratio"""
                        # Simple heuristic: look for "was/were/is/are + past participle"
                        sentences = nltk.sent_tokenize(text)
                        passive_indicators = 0
                        
                        for sentence in sentences:
                            words = nltk.word_tokenize(sentence.lower())
                            pos_tags = nltk.pos_tag(words)
                            
                            for i in range(len(pos_tags) - 1):
                                word, pos = pos_tags[i]
                                next_word, next_pos = pos_tags[i + 1]
                                
                                # Look for be-verb + past participle
                                if word in ['is', 'are', 'was', 'were', 'been'] and next_pos == 'VBN':
                                    passive_indicators += 1
                                    break
                        
                        ratio = passive_indicators / len(sentences) if sentences else 0
                        
                        return {
                            'passive_ratio': ratio,
                            'assessment': 'Appropriate' if ratio < 0.3 else 'Consider more active voice',
                            'passive_sentences': passive_indicators,
                            'total_sentences': len(sentences)
                        }
                        
                    def analyze_transitions(self, text):
                        """Analyze use of transition words and phrases"""
                        
                        transition_words = {
                            'addition': ['furthermore', 'moreover', 'additionally', 'also', 'besides'],
                            'contrast': ['however', 'nevertheless', 'nonetheless', 'whereas', 'although'],
                            'sequence': ['first', 'second', 'finally', 'subsequently', 'then'],
                            'causation': ['therefore', 'consequently', 'thus', 'hence', 'because'],
                            'example': ['specifically', 'particularly', 'namely', 'instance', 'illustration']
                        }
                        
                        text_lower = text.lower()
                        transition_usage = {}
                        
                        for category, words in transition_words.items():
                            count = sum(text_lower.count(word) for word in words)
                            transition_usage[category] = count
                        
                        total_transitions = sum(transition_usage.values())
                        sentences = len(nltk.sent_tokenize(text))
                        transition_density = total_transitions / sentences if sentences > 0 else 0
                        
                        return {
                            'transition_usage': transition_usage,
                            'transition_density': transition_density,
                            'assessment': 'Good flow' if transition_density > 0.1 else 'Consider adding more transitions'
                        }
                        
                    def analyze_paragraph_structure(self, text):
                        """Analyze paragraph structure and organization"""
                        
                        paragraphs = text.split('\n\n')
                        paragraphs = [p.strip() for p in paragraphs if p.strip()]
                        
                        if not paragraphs:
                            return {'assessment': 'No clear paragraph structure detected'}
                        
                        paragraph_lengths = [len(nltk.sent_tokenize(p)) for p in paragraphs]
                        
                        structure_analysis = {
                            'paragraph_count': len(paragraphs),
                            'average_sentences_per_paragraph': np.mean(paragraph_lengths),
                            'paragraph_length_variety': np.std(paragraph_lengths),
                            'length_distribution': paragraph_lengths
                        }
                        
                        # Assessment
                        avg_length = structure_analysis['average_sentences_per_paragraph']
                        if 3 <= avg_length <= 7:
                            assessment = 'Good paragraph length'
                        elif avg_length < 3:
                            assessment = 'Consider longer paragraphs for development'
                        else:
                            assessment = 'Consider shorter paragraphs for readability'
                            
                        structure_analysis['assessment'] = assessment
                        
                        return structure_analysis
                        
                    def analyze_voice_consistency(self, text):
                        """Analyze consistency of voice and perspective"""
                        
                        # Look for person indicators
                        first_person = ['we', 'our', 'us', 'i', 'my', 'mine']
                        third_person = ['the authors', 'this paper', 'this work', 'this study']
                        
                        text_lower = text.lower()
                        
                        first_person_count = sum(text_lower.count(word) for word in first_person)
                        third_person_count = sum(text_lower.count(phrase) for phrase in third_person)
                        
                        total_references = first_person_count + third_person_count
                        
                        if total_references == 0:
                            consistency = 'No clear voice detected'
                        elif first_person_count > third_person_count * 2:
                            consistency = 'Primarily first person (we/our)'
                        elif third_person_count > first_person_count * 2:
                            consistency = 'Primarily third person (the authors/this work)'
                        else:
                            consistency = 'Mixed voice - consider consistency'
                        
                        return {
                            'first_person_usage': first_person_count,
                            'third_person_usage': third_person_count,
                            'voice_consistency': consistency,
                            'recommendation': 'First person acceptable in AI/ML papers' if first_person_count > 0 else 'Third person is more formal'
                        }
                        
                    def assess_clarity(self, text):
                        """Assess overall clarity of writing"""
                        
                        clarity_factors = {
                            'jargon_density': self.calculate_jargon_density(text),
                            'acronym_usage': self.analyze_acronym_usage(text),
                            'sentence_complexity': self.assess_sentence_complexity(text),
                            'logical_flow': self.assess_logical_flow(text)
                        }
                        
                        # Overall clarity score (0-1)
                        clarity_score = self.calculate_overall_clarity(clarity_factors)
                        
                        clarity_factors['overall_score'] = clarity_score
                        clarity_factors['assessment'] = self.interpret_clarity_score(clarity_score)
                        
                        return clarity_factors
                        
                    def calculate_jargon_density(self, text):
                        """Calculate density of specialized jargon"""
                        
                        words = nltk.wor